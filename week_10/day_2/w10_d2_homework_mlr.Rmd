---
title: "R Notebook"
output: html_notebook
---

# Homework - features and elements of multiple regression

## Task 1:Load the housing_prices.csv data set and undertake an initial exploration of the data. You will find details on the data set on the relevant Kaggle page

```{r}
library(tidyverse)
library(GGally)
library(fastDummies)
library(ggfortify)
```

```{r}
house_prices <- read_csv("data/housing_prices.csv")
```
```{r}
glimpse(house_prices)
```
```{r}
summary(house_prices)
```
There are 200 NAs in total_bedrooms, which could contribute to deciding which variable to drop later. 



## Task 2:We expect the total_rooms of houses to be strongly correlated with total_bedrooms. Use ggpairs() to investigate correlations between these two variables.

Step 1: First need to do something with the NAs in total_bedrooms, can't decided whether to impute with mean, or to drop_nas. There is quite a large range of total_bedrooms, but I don't want to lose data so decide to inpute with mean. 

```{r}
house_prices_imputed <- house_prices %>% 
  mutate(total_bedrooms = coalesce(total_bedrooms, mean(total_bedrooms, na.rm = TRUE)))

```

Step 2: Calculate correlation between total_rooms and total_bedrooms. 

These variables are strongly correlated
```{r}
house_prices_imputed %>% 
  summarise(correlation = cor(total_rooms, total_bedrooms))
```

## Task 3: So, we do find significant correlations. Let’s drop total_bedrooms from the dataset, and use only total_rooms going forward

```{r}
house_prices_trim <- house_prices_imputed %>% 
  select(-total_bedrooms)
```

## Task 4: we are interested in developing a regression model for the median_house_value of a house in terms of the possible predictor variables in the dataset.
  - Use ggpairs() to investigate correlations between median_house_value and the predictors (this may take a while to run, don’t worry, make coffee or something).
  
```{r message=FALSE}
ggpairs(house_prices_trim, progress = FALSE) 
```


- Perform further ggplot visualisations of any significant correlations you find.

```{r}
house_prices_trim %>% 
  ggplot(aes(x = median_income, y = median_house_value))+
  geom_point()
```



```{r}
house_prices_trim %>% 
  ggplot(aes(y = ocean_proximity, x = median_house_value))+
  geom_boxplot()
```

## Task 5: Shortly we may try a regression model to fit the categorical predictor ocean_proximity. Investigate the level of ocean_proximity predictors. How many dummy variables do you expect to get from it?

```{r}
house_prices_trim %>% 
  distinct(ocean_proximity)
```
There are five values in the ocean proximity column, so we will have 5 columns for each variable, minus one (which we will have to drop to be the reference variable)

```{r}
house_prices_dummy <- house_prices_trim %>% 
  fastDummies::dummy_cols(select_columns = "ocean_proximity",
                          remove_first_dummy = TRUE,# don't need this column for our model
                          remove_selected_columns = TRUE)

house_prices_dummy
```

## Task 6: Start with simple linear regression. Regress median_house_value on median_income and check the regression diagnostics.

x = explanatory/predictor/independent (median_income)
y = outcome/dependent/response (median_house_value)

```{r}
# formula = y varies with x
model <- lm(median_house_value ~ median_income, 
            data = house_prices_trim)

autoplot(model)
```
plot 1: The residuals appear to be randomly distributed, but within a defined shape. However, the blue line is very close to zero, suggesting that overall there is no pattern to the distribution
plot 2: the residuals fall mostly along a straight line, suggesting that there is normal distribution of residuals
plot 3: the residuals are randomly distributed, but there is some evidence of heteroscedacity, as the data show a defined shape which is funneled towards the right. However, the blue line is relatively straight. 
Plot 4: some outlirs exhibiting more leverage on the residuals. 

```{r}
summary(model)
```


## Task 7: Add another predictor of your choice. Check your assumptions, diagnostics, and interpret the model.

Add in an additional predictor of households, since that seemed to show a high correlation to house value (this makes sense, the more people in the house, the larger it is likely to be. )
```{r}
model2 <- lm(median_house_value ~ median_income + households, 
            data = house_prices_trim)

autoplot(model2)
```
plot 1: checks the assumption that the residuals are independent. In this case, The residuals appear to be randomly scattered, and the blue line is very close to zero, suggesting that overall the residuals are randomly distributed
plot 2: tests that the residuals are randomly distributed around zero, and if they are they align (as these do) along a straight line. 
plot 3: tests the assumption that residuals have constant variation. If the points are randomly distributed, this would indicate that the model fits the data well. IF there is a funnelling effect it would suggest heteroscedacitiy, and that the model doesn't match the data. In this case there is a shape of the data but the values are randomly scattered within the shape, and furthermore, the blue line is relatively straight. 



```{r}
summary(model2)
```
This model would be interpreted as 

house_value = b0 + b1 * median_income + b2 * households
house_value = 37786 + 39799 * median_income + 16.68 * households

The p-value for these 3 coefficients is very low, which is significant and in combination with our diagnostics plot, give us confidence in our assumptions and the model. 


Also, I ran another adding the housing_median_age as a predictor too. 

```{r}
model3 <- lm(median_house_value ~ median_income + housing_median_age, 
            data = house_prices_trim)

autoplot(model3)
```
```{r}
summary(model3)
```
# Homework review additional points

*Feature engineering

```{r}
housing_prices_eng <- house_prices %>% 
  mutate(rooms_per_house = total_rooms/ households,
         people_per_house = population / households,
         rooms_per_person = total_rooms / population)

housing_prices_eng
```

```{r}
housing_prices_eng %>% 
  skimr::skim() %>% 
  view()
```


a lot of our variables are +ve skiewed, = bd reciple for linear model. 

Can log transform

```{r}
housing_prices_eng %>% 
  ggplot(aes(x = median_income))+
  geom_histogram()
```

```{r}
housing_log <- housing_prices_eng %>% 
  select(-c(ocean_proximity, latitude, longitude, housing_median_age)) %>% 
  mutate(across(everything(), log)) %>% # log transofrm everything but the 4 columns above
  rename_with(~ paste0("log_", .x)) %>% 
  bind_cols(housing_prices_eng)

housing_log
```

```{r}
housing_log%>% 
  ggplot(aes(x = log_median_income))+
  geom_histogram()
```

Much more normal

We have a dataste with a richer feature set. 
Now let's start looking for relationships between variables, epsecially between our response/ outcome variable and potential predictors

income vs house value

```{r}
housing_log %>% 
  ggplot()+
  aes(x = median_income, y = median_house_value)+
  geom_point()

```

median_house_va

```{r}
housing_log %>% 
  ggplot(aes(x = ocean_proximity, y = median_house_value))+
  geom_boxplot()
```
```{r}
housing_log %>% 
  ggplot(aes(y = latitude, x = longitude, colour = ocean_proximity))+
geom_point()
```
Bit more feature engineering, let's group these levels so that anything that is near water is named accordingly:

```{r}
housing_ocean <- housing_log %>% 
  mutate(ocean_prox = if_else(
    ocean_proximity %in% c("<1H OCEAN", "NEAR BAY", "NEAR OCEAN"), "NEAR WATER",
    ocean_proximity
  ))

housing_ocean
```

```{r}
housing_ocean %>% 
  ggplot(aes(y = latitude, x = longitude, colour = ocean_prox))+
geom_point()
```
Then we can include this categorical variable in a ggpairs plot, colouring by level

```{r message=FALSE}
housing_ocean %>% 
  select(log_median_house_value, housing_median_age, log_median_income, ocean_prox) %>% 
  ggpairs(aes(colour = ocean_prox, # use this to colour groups
              alpha = 0.5)) # groups semi transparent
```
Trying to build a model to predict medican value, can see that housig age is not a good candidate as coor = 0.044. However, log_median_income has a good correlation. Also, the box plot with grouped ocean_prox shows good correlation too. 

Looking for any predictors which might be correlated to each other. 

Because we put the response/outcome variable first in select() the first column and row are the ones we are mainly interested. Data is grouped by ocean prozitmity -= can see that income and ocean prox eem very related to house value. 

ggpairs plots might give you more ideas for variable engineering!
e.g. this might be what inspied me to group the ocean prox levels. 

Will try the high correlation coeffiecient variable first (income), then will see about adding other predictors (e.g. ocean proximity) 

Now let's try the other variables 

```{r}
housing_ocean %>% 
  select(log_median_house_value, log_total_rooms, log_population, log_households, log_rooms_per_house, log_people_per_house, log_rooms_per_person) %>% 
  ggpairs()
```

House value is first colomn and first row. 

Let's look at first row, none are showing high correlation, although all statistically significant. Therefore, none of these things seems to be particularly strong predictor of house value. 
Not showing many potential predictors. We are seeing highly correlated variables (such as households, populations etc) to each other between the variables, but none to the house_values. 
maybe log_rooms_per_person ( r = 0.256) and log_people_per_household(-0.210)
 
Potential predictors:
income
ocean_prox
maybe rooms per person
maybe people per household


Let's start fitting the model!

  see which predictors work well
  compare model performance
  compare model diagnostics
  add terms iteratively

End goal: explain/predict variation in house prices at the district level (in California)

We want a model that:
  we understand
  we can use to help inform decision making
  explains variation well ("high" r2)
  doesn't break assumption of linear regression
  
Lets start by finding the best wsingle predictor model

```{r}
mod1a <- lm(log_median_house_value ~ log_median_income,
            data = housing_ocean)

autoplot(mod1a)

# without log transofmration
mod1a_nolog <- lm(median_house_value ~ median_income,
            data = housing_ocean)

autoplot(mod1a_nolog)

```

  
  Diagnostics of transofrmed- data model aren't the best in the world - but better than the un-transofmred model. Let's proceed 

```{r}
summary(mod1a)
```

Median income (logged) is a significant predictor of median house value (logged)

How to interpret?? log can make the interpretation a little more difficult. 
  log(median_value) ~ 11.09 + 0.78 * log(median_income)
  "for every unit increase in log(median_income) there is a 0.78 unit increase in log(median_value)

Let's look at another candidate: ocean proximity
  
```{r}
mod1b <- lm(log_median_house_value ~ ocean_prox,
            data = housing_ocean)

autoplot(mod1b)
```
For grouped data, we want to see that each group is normally distributed (within that line) the QQ looks good though. 

```{r}
summary(mod1b)
```

  Reference class: the missing level (INLAND) has been mergend into the intercept ie.e everyone starts at 11.6, and can produc e an estimate for eac hof hte levels provided. 
  
log(median_house_value) = 11.604 * (1.23 * ISLAND) + (0.64 * NEAR WATER)

e.g. island neighbourhoood
                = 11.6 + (1.2 * 1) + (0.6 * 0)
                = 11.6 + 1.2
                =12.8

mod1a (income as predictor) has a higher r2 than mod1b (oceanprox as predictor) - but both do have significant p-values for predictors. 

Going with mod 1a and **that** is one full round of trying to fit a model. 

Here is where we come across a new idea:

  using our model error ("residuals") to help decide what variables to add next. 
  
```{r}
library(modelr)

housing_ocean %>% 
  add_residuals(model = mod1a) %>% 
  select(log_median_house_value, ocean_prox, log_rooms_per_person, log_people_per_house, resid) %>% 
  ggpairs() # resid is how far we are off

```
So ocean proximity seemed to be a very good candidate for explaining some of our model residual. Let's use it as our 2nd predictor in a multiple linear regression model. 

The process:
  try out different candidates for our first predictor
    chose the one with significant p's and highest R^2
  Then added residuals to our dataset, compared these to other protential predictors
    to see which might explain my remaining variance
  Best candidate (i.e. low p-value and high R^2) will be our second predictor
  Then add residuals to the datasets, c ompare these to the remainign predictors
  And so on, iteratively adding predictors to our model. 


```{r}

```






















